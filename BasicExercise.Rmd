---
title: "R Workbook basics"
output:
  html_document:
    keep_md: true
---

# Starting your machine learning project

```{r housing}
# load in the tidyverse package
library(tidyverse) # Utility functions
library(data.table) # Better data.frame
library(rpart) # for regression trees
library(randomForest) # for random forests
# package with the mae function
library(modelr)
```


```{r}
# Using data.table
iowa_table <- fread("data/train.csv") 
iowa_table <- iowa_table[, Condition1 := as.factor(Condition1)]
```

```{r}
str(iowa_table)
summary(iowa_table)
```

```{r}
#str(iowa_table)
#names(iowa_table)
summary(iowa_table[,SalePrice])
```

# Running your first model

```{r}
# print a list of the column names
sort(names(iowa_table), decreasing = F)
# I'll use 'SalePrice', to mimick the notebook
fit <- rpart(
    SalePrice ~  LotArea + YearBuilt + Condition1 + FullBath + BedroomAbvGr + TotRmsAbvGrd,
    data = iowa_table
)
```

```{r}
# plot our regression tree 
plot(fit, uniform=TRUE)
# add text labels & make them 60% as big as they are by default
text(fit, cex=.6)
```

```{r}
print("Making predictions for the first 5 houses:")
#head(iowa_table)

print("The predictions are")
predictions <- predict(fit, head(iowa_table))
print(predictions)

print("Actual price")
actualPrice <- head(iowa_table[,SalePrice])
print(actualPrice)

# Checking the MAE
iowa_mae <- mae(model = fit, data = iowa_table)
print("The MAE is:")
print(iowa_mae)
```

# How do we know if our model is good?

```{r}
# Your turn: split your training data into test & training sets
# split our data so that 30% is in the test set and 70% is in the training set
splitData <- resample_partition(iowa_table, c(test = 0.3, train = 0.7))
# how many cases are in test & training set? 
lapply(splitData, dim)
```

```{r}
# Fit a new model to your training set...
fit2 <- rpart(
    SalePrice ~  LotArea + YearBuilt + Condition1 + FullBath + BedroomAbvGr + TotRmsAbvGrd,
    data = splitData$train
)

# and evaluate it on your test set. Did the error get larger or smaller?
iowa_mae2 <- mae(model = fit2, data = splitData$test)
print("The new MAE is:")
print(iowa_mae2)
```


```{r}
# a function to get the maximum average error for a given max depth. You should pass in
# the target as the name of the target column and the predictors as vector where
# each item in the vector is the name of the column
get_mae <- function(maxdepth, target, predictors, training_data, testing_data){
    
    # turn the predictors & target into a formula to pass to rpart()
    predictors <- paste(predictors, collapse="+")
    formula <- as.formula(paste(target,"~",predictors,sep = ""))
    
    # build our model
    model <- rpart(formula, data = training_data,
                   control = rpart.control(maxdepth = maxdepth))
    # get the mae
    mae <- mae(model, testing_data)
    return(mae)
}
```

# Underfitting/overfitting and improving your model

```{r}
# Your turn: use the get_mae function to find the maxdepth that leads to the 
# lowest mean average error for this dataset
target <- 'SalePrice'
predictors <- c(
  'LotArea', 'YearBuilt', 'Condition1', 'FullBath',
  'BedroomAbvGr', 'TotRmsAbvGrd'
)

for(i in 1:10) {
    mae <- get_mae(
        maxdepth = i, target = target, predictors = predictors,
        training_data = splitData$train, testing_data = splitData$test
    )
    print(paste("Maxdepth: ",i," | MAE: ",mae))
}
```


```{r}

# Your turn: Train a random forest using the same features as you used
# to train your original decision tree.
# Check out the MAE. Did you see an improvement over your original model?
# Fit a new model to your training set...
fitRR <- randomForest(
    SalePrice ~  LotArea + YearBuilt + Condition1 + FullBath + BedroomAbvGr + TotRmsAbvGrd,
    data = splitData$train
)

# and evaluate it on your test set. Did the error get larger or smaller?
iowa_mae_RR <- mae(model = fitRR, data = splitData$test)
print("The new MAE is:")
print(iowa_mae_RR)
```

```{r}
print(iowa_mae2 - iowa_mae_RR)
# However, for this to get easier to see, it'd need to be converted into a percentage of sorts...
```
# Testing on the test data

```{r}
iowa_test <- fread("data/test.csv") 
iowa_test <- iowa_test[, Condition1 := as.factor(Condition1)]
iowa_test <- iowa_test[, SalePrice := predict(fitRR, .SD)]
```

# Making it ready for submission
This is the lesson 8, not sure why it isn't properly done on this notebook...



```{r}
# create a dataframe with our results
my_submission <- data.table(
  'Id' = as.integer(iowa_test$Id), 'SalePrice' = iowa_test$SalePrice
)
# save our file
fwrite(my_submission, 'submission.csv')
```
